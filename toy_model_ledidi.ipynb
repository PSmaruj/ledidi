{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader,SubsetRandomSampler,ConcatDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home1/smaruj/AkitaMini-pytorch\")  # Add the directory where \"ledidi\" is located\n",
    "from model import SeqNN\n",
    "from helper import plot_map, from_upper_triu, upper_triangular_to_vector_skip_diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_234430/ipykernel_1521885/2425661507.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home1/smaruj/AkitaMini-pytorch/best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SeqNN()\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('/home1/smaruj/AkitaMini-pytorch/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqNN(\n",
       "  (stochastic_reverse_complement): StochasticReverseComplement()\n",
       "  (stochastic_shift): StochasticShift()\n",
       "  (conv_block_1): ConvBlock(\n",
       "    (conv): Conv1d(4, 96, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n",
       "    (batch_norm): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (pool): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_tower): ConvTower(\n",
       "    (conv_tower): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (2): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): ReLU()\n",
       "      (5): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (6): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): ReLU()\n",
       "      (9): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (10): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (12): ReLU()\n",
       "      (13): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (14): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (15): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (16): ReLU()\n",
       "      (17): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (18): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (19): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (20): ReLU()\n",
       "      (21): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (22): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (23): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (residual1d_block1): ResidualDilatedBlock1D(\n",
       "    (relu1): ReLU()\n",
       "    (conv1): Conv1d(96, 48, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (bn1): BatchNorm1d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "    (conv2): Conv1d(48, 96, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (bn2): BatchNorm1d(96, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (conv_reduce): ConvBlockReduce(\n",
       "    (layers): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Conv1d(96, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (2): BatchNorm1d(64, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (one_to_two): OneToTwo()\n",
       "  (concat_dist): ConcatDist2D()\n",
       "  (conv2d_block): Conv2DBlock(\n",
       "    (block): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (symmetrize_2d): Symmetrize2D()\n",
       "  (residual2d_block1): DilatedResidualBlock2D(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(24, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (symmetrize): Symmetrize2D()\n",
       "  )\n",
       "  (residual2d_block2): DilatedResidualBlock2D(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(24, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "    (bn2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (symmetrize): Symmetrize2D()\n",
       "  )\n",
       "  (residual2d_block3): DilatedResidualBlock2D(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "    (bn1): BatchNorm2d(24, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), dilation=(4, 4), bias=False)\n",
       "    (bn2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (symmetrize): Symmetrize2D()\n",
       "  )\n",
       "  (residual2d_block4): DilatedResidualBlock2D(\n",
       "    (relu): ReLU()\n",
       "    (conv1): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False)\n",
       "    (bn1): BatchNorm2d(24, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), dilation=(7, 7), bias=False)\n",
       "    (bn2): BatchNorm2d(48, eps=0.001, momentum=0.0735, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (symmetrize): Symmetrize2D()\n",
       "  )\n",
       "  (cropping_2d): Cropping2D()\n",
       "  (upper_tri): UpperTri()\n",
       "  (final): Final(\n",
       "    (dense): Linear(in_features=48, out_features=1, bias=True)\n",
       "  )\n",
       "  (switch_reverse_triu): SwitchReverseTriu()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode (important for inference)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SeqNN                                    [2, 1, 1953]              --\n",
       "├─StochasticReverseComplement: 1-1       [2, 4, 32768]             --\n",
       "├─StochasticShift: 1-2                   [2, 4, 32768]             --\n",
       "├─ConvBlock: 1-3                         [2, 96, 4096]             --\n",
       "│    └─Conv1d: 2-1                       [2, 96, 32768]            4,224\n",
       "│    └─BatchNorm1d: 2-2                  [2, 96, 32768]            192\n",
       "│    └─MaxPool1d: 2-3                    [2, 96, 4096]             --\n",
       "├─ConvTower: 1-4                         [2, 96, 64]               --\n",
       "│    └─Sequential: 2-4                   [2, 96, 64]               --\n",
       "│    │    └─ReLU: 3-1                    [2, 96, 4096]             --\n",
       "│    │    └─Conv1d: 3-2                  [2, 96, 4096]             46,080\n",
       "│    │    └─BatchNorm1d: 3-3             [2, 96, 4096]             192\n",
       "│    │    └─MaxPool1d: 3-4               [2, 96, 2048]             --\n",
       "│    │    └─ReLU: 3-5                    [2, 96, 2048]             --\n",
       "│    │    └─Conv1d: 3-6                  [2, 96, 2048]             46,080\n",
       "│    │    └─BatchNorm1d: 3-7             [2, 96, 2048]             192\n",
       "│    │    └─MaxPool1d: 3-8               [2, 96, 1024]             --\n",
       "│    │    └─ReLU: 3-9                    [2, 96, 1024]             --\n",
       "│    │    └─Conv1d: 3-10                 [2, 96, 1024]             46,080\n",
       "│    │    └─BatchNorm1d: 3-11            [2, 96, 1024]             192\n",
       "│    │    └─MaxPool1d: 3-12              [2, 96, 512]              --\n",
       "│    │    └─ReLU: 3-13                   [2, 96, 512]              --\n",
       "│    │    └─Conv1d: 3-14                 [2, 96, 512]              46,080\n",
       "│    │    └─BatchNorm1d: 3-15            [2, 96, 512]              192\n",
       "│    │    └─MaxPool1d: 3-16              [2, 96, 256]              --\n",
       "│    │    └─ReLU: 3-17                   [2, 96, 256]              --\n",
       "│    │    └─Conv1d: 3-18                 [2, 96, 256]              46,080\n",
       "│    │    └─BatchNorm1d: 3-19            [2, 96, 256]              192\n",
       "│    │    └─MaxPool1d: 3-20              [2, 96, 128]              --\n",
       "│    │    └─ReLU: 3-21                   [2, 96, 128]              --\n",
       "│    │    └─Conv1d: 3-22                 [2, 96, 128]              46,080\n",
       "│    │    └─BatchNorm1d: 3-23            [2, 96, 128]              192\n",
       "│    │    └─MaxPool1d: 3-24              [2, 96, 64]               --\n",
       "├─ResidualDilatedBlock1D: 1-5            [2, 96, 64]               --\n",
       "│    └─ReLU: 2-5                         [2, 96, 64]               --\n",
       "│    └─Conv1d: 2-6                       [2, 48, 64]               13,824\n",
       "│    └─BatchNorm1d: 2-7                  [2, 48, 64]               96\n",
       "│    └─ReLU: 2-8                         [2, 48, 64]               --\n",
       "│    └─Conv1d: 2-9                       [2, 96, 64]               4,608\n",
       "│    └─BatchNorm1d: 2-10                 [2, 96, 64]               192\n",
       "│    └─Dropout: 2-11                     [2, 96, 64]               --\n",
       "├─ConvBlockReduce: 1-6                   [2, 64, 64]               --\n",
       "│    └─Sequential: 2-12                  [2, 64, 64]               --\n",
       "│    │    └─ReLU: 3-25                   [2, 96, 64]               --\n",
       "│    │    └─Conv1d: 3-26                 [2, 64, 64]               30,720\n",
       "│    │    └─BatchNorm1d: 3-27            [2, 64, 64]               128\n",
       "│    │    └─ReLU: 3-28                   [2, 64, 64]               --\n",
       "├─OneToTwo: 1-7                          [2, 64, 64, 64]           --\n",
       "├─ConcatDist2D: 1-8                      [2, 65, 64, 64]           --\n",
       "├─Conv2DBlock: 1-9                       [2, 48, 64, 64]           --\n",
       "│    └─Sequential: 2-13                  [2, 48, 64, 64]           --\n",
       "│    │    └─ReLU: 3-29                   [2, 65, 64, 64]           --\n",
       "│    │    └─Conv2d: 3-30                 [2, 48, 64, 64]           28,080\n",
       "│    │    └─BatchNorm2d: 3-31            [2, 48, 64, 64]           96\n",
       "├─Symmetrize2D: 1-10                     [2, 48, 64, 64]           --\n",
       "├─DilatedResidualBlock2D: 1-11           [2, 48, 64, 64]           --\n",
       "│    └─ReLU: 2-14                        [2, 48, 64, 64]           --\n",
       "│    └─Conv2d: 2-15                      [2, 24, 64, 64]           10,368\n",
       "│    └─BatchNorm2d: 2-16                 [2, 24, 64, 64]           48\n",
       "│    └─ReLU: 2-17                        [2, 24, 64, 64]           --\n",
       "│    └─Conv2d: 2-18                      [2, 48, 64, 64]           1,152\n",
       "│    └─BatchNorm2d: 2-19                 [2, 48, 64, 64]           96\n",
       "│    └─Dropout2d: 2-20                   [2, 48, 64, 64]           --\n",
       "│    └─Symmetrize2D: 2-21                [2, 48, 64, 64]           --\n",
       "├─DilatedResidualBlock2D: 1-12           [2, 48, 64, 64]           --\n",
       "│    └─ReLU: 2-22                        [2, 48, 64, 64]           --\n",
       "│    └─Conv2d: 2-23                      [2, 24, 64, 64]           10,368\n",
       "│    └─BatchNorm2d: 2-24                 [2, 24, 64, 64]           48\n",
       "│    └─ReLU: 2-25                        [2, 24, 64, 64]           --\n",
       "│    └─Conv2d: 2-26                      [2, 48, 64, 64]           1,152\n",
       "│    └─BatchNorm2d: 2-27                 [2, 48, 64, 64]           96\n",
       "│    └─Dropout2d: 2-28                   [2, 48, 64, 64]           --\n",
       "│    └─Symmetrize2D: 2-29                [2, 48, 64, 64]           --\n",
       "├─DilatedResidualBlock2D: 1-13           [2, 48, 64, 64]           --\n",
       "│    └─ReLU: 2-30                        [2, 48, 64, 64]           --\n",
       "│    └─Conv2d: 2-31                      [2, 24, 64, 64]           10,368\n",
       "│    └─BatchNorm2d: 2-32                 [2, 24, 64, 64]           48\n",
       "│    └─ReLU: 2-33                        [2, 24, 64, 64]           --\n",
       "│    └─Conv2d: 2-34                      [2, 48, 64, 64]           1,152\n",
       "│    └─BatchNorm2d: 2-35                 [2, 48, 64, 64]           96\n",
       "│    └─Dropout2d: 2-36                   [2, 48, 64, 64]           --\n",
       "│    └─Symmetrize2D: 2-37                [2, 48, 64, 64]           --\n",
       "├─DilatedResidualBlock2D: 1-14           [2, 48, 64, 64]           --\n",
       "│    └─ReLU: 2-38                        [2, 48, 64, 64]           --\n",
       "│    └─Conv2d: 2-39                      [2, 24, 64, 64]           10,368\n",
       "│    └─BatchNorm2d: 2-40                 [2, 24, 64, 64]           48\n",
       "│    └─ReLU: 2-41                        [2, 24, 64, 64]           --\n",
       "│    └─Conv2d: 2-42                      [2, 48, 64, 64]           1,152\n",
       "│    └─BatchNorm2d: 2-43                 [2, 48, 64, 64]           96\n",
       "│    └─Dropout2d: 2-44                   [2, 48, 64, 64]           --\n",
       "│    └─Symmetrize2D: 2-45                [2, 48, 64, 64]           --\n",
       "├─Cropping2D: 1-15                       [2, 48, 64, 64]           --\n",
       "├─UpperTri: 1-16                         [2, 48, 1953]             --\n",
       "├─Final: 1-17                            [2, 1, 1953]              --\n",
       "│    └─Linear: 2-46                      [2, 1953, 1]              49\n",
       "├─SwitchReverseTriu: 1-18                [2, 1, 1953]              --\n",
       "==========================================================================================\n",
       "Total params: 406,497\n",
       "Trainable params: 406,497\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.63\n",
       "==========================================================================================\n",
       "Input size (MB): 1.05\n",
       "Forward/backward pass size (MB): 169.93\n",
       "Params size (MB): 1.63\n",
       "Estimated Total Size (MB): 172.61\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(2, 4, 32768), col_names=[\"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.load(\"/scratch1/smaruj/ledidi_targets/X.pt\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_vector_tensor = torch.load(\"/scratch1/smaruj/ledidi_targets/modified_vector.pt\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def get_affected_bins(self, changed_indices):\n",
    "    #     \"\"\"\n",
    "    #     Determine which bins contain changed indices.\n",
    "    #     Also include ±1 bin for safety.\n",
    "    #     \"\"\"\n",
    "    #     slice_start, slice_end = changed_indices\n",
    "    \n",
    "    #     # Compute correct bin indices\n",
    "    #     start_bin = slice_start // self.bin_size\n",
    "    #     end_bin = slice_end // self.bin_size  # Ensure proper bin inclusion\n",
    "        \n",
    "    #     affected_bins = set(range(start_bin, end_bin + 1))  # Directly affected bins\n",
    "\n",
    "    #     # Include ±1 neighboring bins\n",
    "    #     if start_bin > 0:\n",
    "    #         affected_bins.add(start_bin - 1)\n",
    "    #     if end_bin < self.num_bins - 1:\n",
    "    #         affected_bins.add(end_bin + 1)\n",
    "\n",
    "    #     return sorted(affected_bins)  # Ensure ordered output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure the local, forked ledidi is used\n",
    "# not the one installed using pip\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/home1/smaruj/ledidi\")  # Add the directory where \"ledidi\" is located\n",
    "from ledidi import Ledidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, modified_vector_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtype, modified_vector_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper = Ledidi(model, (1, 32768, 4), verbose=True, batch_size=1,\n",
    "#                  input_loss=torch.nn.L1Loss(reduction='sum'), \n",
    "#                  output_loss=torch.nn.MSELoss(), # default losses\n",
    "#                  max_iter=20000,\n",
    "#                  early_stopping_iter=20000\n",
    "#                  ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 10752\n",
    "end_index = 11264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = end_index - start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients enabled for weights: True\n",
      "Model in train mode: True\n",
      "Weights shape torch.Size([1, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "wrapper = Ledidi(model, verbose=True, batch_size=10,\n",
    "                 input_loss=torch.nn.L1Loss(reduction='sum'), \n",
    "                 output_loss=torch.nn.L1Loss(reduction='sum'),\n",
    "                 max_iter=20000,\n",
    "                 early_stopping_iter=2000,\n",
    "                 slice_length=seq_length, \n",
    "                 slice_index=21,\n",
    "                 use_semifreddo=True,\n",
    "                #  return_history=True\n",
    "                 ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_torch = X[:,:,start_index : end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l_flank = X[:,:,start_index - 512*2 : start_index]\n",
    "X_r_flank = X[:,:,end_index : end_index + 512*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# x_bar, history = wrapper.fit_transform(X, modified_vector_tensor)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# if use_semifreddo=True\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m x_bar \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodified_vector_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_l_flank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_l_flank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_r_flank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_r_flank\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ledidi/ledidi/ledidi.py:288\u001b[0m, in \u001b[0;36mLedidi.fit_transform\u001b[0;34m(self, X, y_bar, X_l_flank, X_r_flank)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_semifreddo:\n\u001b[1;32m    287\u001b[0m     semifreddo_model \u001b[38;5;241m=\u001b[39m Semifreddo(flanked_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, saved_out_path, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43msemifreddo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X)[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget]\n",
      "File \u001b[0;32m~/AkitaMini-pytorch/semifreddo_model.py:31\u001b[0m, in \u001b[0;36mSemifreddo.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(\"x[:, :, self.edited_index]\\n\", x[:, :, 21])\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# print(\"sub_x.squeeze(-1)\\n\", sub_x.squeeze(-1))\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medited_index\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m sub_x[:,:,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#2 becaise, there are 5 bins, we need the middle one\u001b[39;00m\n\u001b[1;32m     32\u001b[0m x\u001b[38;5;241m.\u001b[39mrequires_grad_()  \u001b[38;5;66;03m# Explicitly tell PyTorch to track gradients\u001b[39;00m\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mresidual1d_block1(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "# x_bar, history = wrapper.fit_transform(X, modified_vector_tensor)\n",
    "# if use_semifreddo=True\n",
    "x_bar = wrapper.fit_transform(slice_torch, modified_vector_tensor, X_l_flank=X_l_flank, X_r_flank=X_r_flank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if use_semifreddo=False\n",
    "x_bar = wrapper.fit_transform(X, modified_vector_tensor, X_l_flank=X_l_flank, X_r_flank=X_r_flank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "\n",
    "scaled_output_loss = numpy.array(history['output_loss']) * 10\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(history['input_loss'], c='0.7', label=\"Input Loss\")\n",
    "plt.plot(scaled_output_loss, c='0.3', label=\"Scaled Output Loss\")\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "seaborn.despine(left=True, bottom=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X.clone()\n",
    "X_new = X_new.repeat(10, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new[:,:,start_index:end_index] = x_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"sequence\", i)\n",
    "    print(\"edits =\", (X[0,:,:] != X_new[i,:,:]).sum() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l_flank.repeat(10, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,:,start_index : end_index] = x_bar[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"sequence\", i)\n",
    "    print(\"edits =\", (x_bar[i,:,:] != X).sum() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    # q_prediction = model(x_bar)\n",
    "    q_prediction = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(from_upper_triu(q_prediction[0], matrix_len=64, num_diags=2), vmin=-0.6, vmax=0.6, palette=\"RdBu_r\", width=5, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(i)\n",
    "    plot_map(from_upper_triu(q_prediction[i], matrix_len=64, num_diags=2), vmin=-0.6, vmax=0.6, palette=\"RdBu_r\", width=5, height=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
